{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.audio import SignalDistortionRatio as SDR\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torchaudio\n",
    "import wandb\n",
    "from audiotools import AudioSignal\n",
    "from audiotools.data.datasets import AudioDataset, AudioLoader\n",
    "import dac\n",
    "from dac.nn.layers import snake, Snake1d\n",
    "from dac.nn.loss import *\n",
    "from flatten_dict import flatten, unflatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Device setup ###\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath(\" \"))\n",
    "\n",
    "### Argument parsing ###\n",
    "\n",
    "config = json.load(open(os.path.join(dir_path, 'config.json')))\n",
    "\n",
    "# apply custom activation function\n",
    "if config[\"custom_act_func\"] == \"silu\":\n",
    "    custom_act_func = nn.SiLU()\n",
    "if config[\"custom_act_func\"] == \"selu\":\n",
    "    custom_act_func = nn.SELU()\n",
    "if config[\"custom_act_func\"] == \"tanh\":\n",
    "    custom_act_func = nn.Tanh()\n",
    "\n",
    "### Wandb setup ###\n",
    "if config[\"use_wandb\"]:\n",
    "    wandb.init(\n",
    "        project=\"Audio-project\",\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(clean : AudioSignal, noise : AudioSignal): return clean.clone().mix(noise, snr=config[\"snr\"])\n",
    "\n",
    "def pretty_print_output(output : dict):\n",
    "    pretty_output = {k: (v.detach().cpu().numpy() if torch.is_tensor(v) else v) for k, v in output.items()}\n",
    "    pretty_output_str = {k: np.array_str(v, precision=4, suppress_small=True) if isinstance(v, np.ndarray) else v for k, v in pretty_output.items()}\n",
    "    for key, value in pretty_output_str.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "def count_files(directory):\n",
    "    file_count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        file_count += len(files)\n",
    "    return file_count\n",
    "\n",
    "def change_activation_function(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, Snake1d):\n",
    "            setattr(model, name, custom_act_func)\n",
    "        else:\n",
    "            change_activation_function(module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioloaders = {\n",
    "    \"voice\": AudioLoader(sources=[config[\"voice_folder\"]], shuffle=False),\n",
    "    \"noise\": AudioLoader(sources=[config[\"noise_folder\"]], shuffle=True)\n",
    "    }\n",
    "\n",
    "# Use different dataset for saving samples for longer audio samples\n",
    "dataset_val = AudioDataset(audioloaders,n_examples=config[\"n_samples\"], sample_rate=config[\"sample_rate\"], duration = 5.0)\n",
    "dataset = AudioDataset(audioloaders,n_examples=config[\"n_samples\"], sample_rate=config[\"sample_rate\"], duration = 0.5)\n",
    "dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=dataset.collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DAC model ###\n",
    "if config[\"use_pretrained\"]:\n",
    "    model_path = dac.utils.download(model_type=\"44khz\")\n",
    "    generator = dac.DAC.load(model_path)\n",
    "else:\n",
    "    generator = dac.DAC()\n",
    "\n",
    "if config[\"use_custom_activation\"]:\n",
    "    change_activation_function(generator)\n",
    "\n",
    "generator = generator.to(device)\n",
    "\n",
    "optimizer_gen = optim.AdamW(generator.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], config[\"beta2\"]))\n",
    "\n",
    "### Mean Opinion Score models ###\n",
    "if config[\"use_mos\"]:\n",
    "    from torchaudio.pipelines import SQUIM_OBJECTIVE, SQUIM_SUBJECTIVE\n",
    "    subjective_model = SQUIM_SUBJECTIVE.get_model().to(device)\n",
    "    objective_model = SQUIM_OBJECTIVE.get_model().to(device)\n",
    "\n",
    "### Hifi-plus-plus discriminators ###\n",
    "if config[\"hpp_disc\"]:\n",
    "    from hifiplusplus_discriminator import *\n",
    "    MSD = MultiScaleDiscriminator().to(device).train()\n",
    "    MPD = MultiPeriodDiscriminator().to(device).train()\n",
    "    optimizer_msd = optim.AdamW(MSD.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], config[\"beta2\"]))\n",
    "    optimizer_mpd = optim.AdamW(MPD.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], config[\"beta2\"]))\n",
    "\n",
    "#### Descript audio codec discriminator ###\n",
    "if config[\"dac_disc\"]:\n",
    "    dac_disc = dac.model.Discriminator().to(device).train()\n",
    "    optimizer_dac_disc = optim.AdamW(dac_disc.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], config[\"beta2\"]))\n",
    "\n",
    "### Schedulers ###\n",
    "scheduler = StepLR(optimizer_gen, step_size=30, gamma=0.1) if config[\"use_scheduler\"] else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_loss = MultiScaleSTFTLoss().to(device)\n",
    "mel_loss = MelSpectrogramLoss().to(device)\n",
    "waveform_loss = L1Loss().to(device)\n",
    "sisdr_loss = SISDRLoss().to(device)\n",
    "sdr_loss = SDR().to(device)\n",
    "gan_loss = GANLoss(dac_disc).to(device) if config[\"dac_disc\"] else None\n",
    "\n",
    "loss_weights = {\n",
    "    \"mel/loss\": 45.0,  \n",
    "    \"adv/feat_loss\": 5.0,\n",
    "    \"adv/gen_loss\": 5.0, \n",
    "    \"vq/commitment_loss\": 0.5, \n",
    "    \"vq/codebook_loss\": 1.0, \n",
    "    \"waveform/loss\": 45.0,\n",
    "    \"stft/loss\": 1.0,  \n",
    "    \"sisdr/loss\": 1.0,\n",
    "    \"sdr/loss\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(noisy_signal : AudioSignal, signal : AudioSignal):\n",
    "\n",
    "    # Set models to train mode\n",
    "    generator.train()\n",
    "\n",
    "    output = {}\n",
    "\n",
    "    # Generator Forward Pass\n",
    "    out = generator(noisy_signal.audio_data, noisy_signal.sample_rate)\n",
    "    recons = AudioSignal(out[\"audio\"], noisy_signal.sample_rate)\n",
    "    commitment_loss = out[\"vq/commitment_loss\"]\n",
    "    codebook_loss = out[\"vq/codebook_loss\"]\n",
    "\n",
    "    # Hif-plus-plus discriminators\n",
    "    if config[\"hpp_disc\"]:\n",
    "\n",
    "        # Multi Period Discriminator\n",
    "        real_mpd, gen_mpd, _, _ = MPD(signal.audio_data, recons.clone().detach().audio_data)\n",
    "        output[\"adv/disc_loss_mpd\"], _, _ = discriminator_loss(real_mpd, gen_mpd)\n",
    "\n",
    "        # Multi Scale Discriminator\n",
    "        real_msd, gen_msd, _, _ = MSD(signal.audio_data, recons.clone().detach().audio_data)\n",
    "        output[\"adv/disc_loss_msd\"], _, _ = discriminator_loss(real_msd, gen_msd)\n",
    "\n",
    "        # Update Discriminators\n",
    "        output[\"adv/disc_loss_mpd\"].backward()\n",
    "        optimizer_mpd.step()\n",
    "        optimizer_mpd.zero_grad()\n",
    "\n",
    "        output[\"adv/disc_loss_msd\"].backward()\n",
    "        optimizer_msd.step()\n",
    "        optimizer_msd.zero_grad()\n",
    "\n",
    "        # Feature and Generator Losses\n",
    "        _, gen_mpd, feat_real_mpd, feat_gen_mpd = MPD(signal.audio_data, recons.audio_data)\n",
    "        feat_loss_mpd = feature_loss(feat_real_mpd, feat_gen_mpd)\n",
    "        mpd_gen_loss, _ = generator_loss(gen_mpd)\n",
    "        \n",
    "        _, gen_msd, feat_real_msd, feat_gen_msd = MSD(signal.audio_data, recons.audio_data)    \n",
    "        feat_loss_msd = feature_loss(feat_real_msd, feat_gen_msd)\n",
    "        msd_gen_loss, _ = generator_loss(gen_msd)\n",
    "        \n",
    "        output[\"adv/feat_loss\"] = feat_loss_msd + feat_loss_mpd\n",
    "        output[\"adv/gen_loss\"] = msd_gen_loss + mpd_gen_loss\n",
    "    \n",
    "\n",
    "    # Descript audio codec discriminator\n",
    "    if config[\"dac_disc\"]:\n",
    "        output[\"adv/disc_loss\"] = gan_loss.discriminator_loss(recons, signal)\n",
    "        output[\"adv/disc_loss\"].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(dac_disc.parameters(), 1e3)\n",
    "        optimizer_dac_disc.step()\n",
    "        optimizer_dac_disc.zero_grad()\n",
    "        \n",
    "        # Generator Losses\n",
    "        output[\"adv/gen_loss\"], output[\"adv/feat_loss\"] = gan_loss.generator_loss(recons, signal)\n",
    "\n",
    "    # Other Losses\n",
    "    output[\"stft/loss\"] = stft_loss(recons, signal)\n",
    "    output[\"mel/loss\"] = mel_loss(recons, signal)\n",
    "    output[\"waveform/loss\"] = waveform_loss(recons, signal)\n",
    "    output[\"sisdr/loss\"] = sisdr_loss(recons.audio_data, signal.audio_data)\n",
    "    output[\"sdr/loss\"] = -sdr_loss(recons.audio_data, signal.audio_data)\n",
    "    output[\"vq/commitment_loss\"] = commitment_loss\n",
    "    output[\"vq/codebook_loss\"] = codebook_loss\n",
    "    output[\"loss\"] = sum([v * output[k] for k, v in loss_weights.items() if k in output])\n",
    "\n",
    "    # Update Generator\n",
    "    output[\"loss\"].backward()\n",
    "    torch.nn.utils.clip_grad_norm_(generator.parameters(), 1e3)\n",
    "    optimizer_gen.step()\n",
    "    scheduler.step() if config[\"use_scheduler\"] else None\n",
    "    optimizer_gen.zero_grad()\n",
    "\n",
    "    # Logging\n",
    "    log_data = {k: v.item() if torch.is_tensor(v) else v for k, v in output.items()}\n",
    "    \n",
    "    if config[\"use_wandb\"]:\n",
    "        wandb.log(log_data)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_loop(noisy_signal : AudioSignal,\n",
    "             signal : AudioSignal):\n",
    "    generator.eval()\n",
    "\n",
    "    # Create samples\n",
    "    print(\"\\nValidation:\\n\") if config[\"do_print\"] else None\n",
    "    output = {}\n",
    "    out = generator(noisy_signal.audio_data, noisy_signal.sample_rate)\n",
    "    recons = AudioSignal(out[\"audio\"], noisy_signal.sample_rate)\n",
    "\n",
    "    # Get perceptual metrics\n",
    "    if config[\"use_mos\"]:\n",
    "        recons_16khz, signal_16khz = torchaudio.functional.resample(recons.audio_data, config[\"sample_rate\"], 16000).squeeze(1), torchaudio.functional.resample(signal.audio_data, config[\"sample_rate\"], 16000).squeeze(1)\n",
    "        output[\"MOS\"] = subjective_model(recons_16khz, signal_16khz).mean()\n",
    "        stoi, pesq, si_sdr = objective_model(recons_16khz)\n",
    "        output[\"STOI\"],output[\"PESQ\"], output[\"SI-SDR\"] = stoi.mean(), pesq.mean(), si_sdr.mean()\n",
    "    \n",
    "    # Log and print\n",
    "    log_data = {k: v.item() if torch.is_tensor(v) else v for k, v in output.items()}\n",
    "    wandb.log(log_data) if config[\"use_wandb\"] else None\n",
    "    pretty_print_output(log_data) if config[\"do_print\"] else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def save_samples(epoch : int, i : int):\n",
    "    generator.eval()\n",
    "\n",
    "    # Create samples\n",
    "    noise, clean = dataset_val[i][\"noise\"][\"signal\"], dataset_val[i][\"voice\"][\"signal\"]\n",
    "    noise, clean = noise.to(device), clean.to(device)\n",
    "    noisy_signal = add_noise(clean, noise)\n",
    "    out = generator(noisy_signal.audio_data.to(device), noisy_signal.sample_rate)[\"audio\"]\n",
    "    recons = AudioSignal(out, 44100)\n",
    "\n",
    "    # Define paths\n",
    "    recons_path = f\"./output/recons_e{epoch}b{i}.wav\"\n",
    "    noisy_path = f\"./output/noisy_e{epoch}b{i}.wav\"\n",
    "    clean_path = f\"./output/clean_e{epoch}b{i}.wav\"\n",
    "\n",
    "    # Write to disk\n",
    "    recons.cpu().write(recons_path)\n",
    "    noisy_signal.cpu().write(noisy_path)\n",
    "    clean.cpu().write(clean_path)\n",
    "\n",
    "    print(f\"Saved samples to {recons_path}, {noisy_path} and {clean_path}\") if config[\"do_print\"] else None\n",
    "    \n",
    "    wandb.log({\"Reconstructed Audio\": wandb.Audio(recons_path, caption=f\"Reconstructed Epoch {epoch} Batch {i}\"),\n",
    "               \"Noisy Audio\": wandb.Audio(noisy_path, caption=f\"Noisy Epoch {epoch} Batch {i}\"),\n",
    "               \"Clean Audio\": wandb.Audio(clean_path, caption=f\"Clean Epoch {epoch} Batch {i}\")}) if config[\"use_wandb\"] else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training\") if config[\"do_print\"] else None\n",
    "\n",
    "for epoch in range(config[\"n_epochs\"]):\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Load data and add noise\n",
    "        signal, noise = batch[\"voice\"][\"signal\"].to(device), batch[\"noise\"][\"signal\"].to(device)\n",
    "        noisy_signal = add_noise(signal, noise)\n",
    "\n",
    "        out = train_loop(noisy_signal, signal)\n",
    "        \n",
    "        if (i%config[\"val_interval\"] == 0):\n",
    "\n",
    "            if config[\"do_print\"]:\n",
    "                print(f\"\\nBatch {i}:\\n\")\n",
    "                pretty_print_output(out)\n",
    "            \n",
    "            # Sample and log\n",
    "            save_samples(epoch, i)\n",
    "            val_loop(noisy_signal, signal)\n",
    "\n",
    "        # Save state dict\n",
    "        if (i%config[\"save_state_dict_interval\"] == 0) & (i != 0):\n",
    "            if config[\"save_state_dict\"]:\n",
    "                generator.eval()\n",
    "                torch.save(generator.state_dict(), f\"./models/dac_hifi_e{epoch}_it{i}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
